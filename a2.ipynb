{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YUbfHXbGZ4N8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Grid size and actions\n",
        "grid_size = 5\n",
        "num_states = grid_size * grid_size\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "num_actions = len(actions)\n",
        "action_indices = {a: i for i, a in enumerate(actions)}\n",
        "\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.95\n",
        "\n",
        "# Convert row and column to single index and back\n",
        "def to_1d(row, col):\n",
        "    return row * grid_size + col\n",
        "\n",
        "def to_2d(state):\n",
        "    return divmod(state, grid_size)\n",
        "\n",
        "# Special square definitions\n",
        "blue = to_1d(0, 1)\n",
        "green = to_1d(0, 3)\n",
        "red = to_1d(4, 1)\n",
        "yellow = to_1d(2, 3)\n",
        "\n",
        "# Move logic\n",
        "def step(state, action):\n",
        "    \"\"\"\n",
        "    Returns the next state and reward after taking an action.\n",
        "    Handles jump squares and edge penalties.\n",
        "    \"\"\"\n",
        "    row, col = to_2d(state)\n",
        "\n",
        "    # Special jumps(SP)\n",
        "    if state == blue:\n",
        "        return red, 5.0\n",
        "    if state == green:\n",
        "        target = yellow if np.random.rand() < 0.5 else red\n",
        "        return target, 2.5\n",
        "\n",
        "    # Attempt normal moves\n",
        "    if action == 'up':\n",
        "        next_row, next_col = max(row - 1, 0), col\n",
        "    elif action == 'down':\n",
        "        next_row, next_col = min(row + 1, grid_size - 1), col\n",
        "    elif action == 'left':\n",
        "        next_row, next_col = row, max(col - 1, 0)\n",
        "    elif action == 'right':\n",
        "        next_row, next_col = row, min(col + 1, grid_size - 1)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid action: {action}\")\n",
        "\n",
        "    # Check attempt off grid\n",
        "    if (next_row == row and next_col == col) and state not in (red, yellow):\n",
        "        return state, -0.5\n",
        "    else:\n",
        "        next_state = to_1d(next_row, next_col)\n",
        "        return next_state, 0.0  # normal move\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import solve\n",
        "\n",
        "# Transition and reward setup under a random policy\n",
        "p_pi = np.zeros((num_states, num_states))\n",
        "r_pi = np.zeros(num_states)\n",
        "\n",
        "# Equal chance for each action\n",
        "action_probs = [0.25] * num_actions\n",
        "\n",
        "for s in range(num_states):\n",
        "    if s == blue:\n",
        "        # Blue square: always jumps to red with reward +5\n",
        "        p_pi[s, red] = 1.0\n",
        "        r_pi[s] = 5.0\n",
        "\n",
        "    elif s == green:\n",
        "        # Green square: 5050 jump to red or yellow, reward is 2.5\n",
        "        p_pi[s, red] = 0.5\n",
        "        p_pi[s, yellow] = 0.5\n",
        "        r_pi[s] = 2.5\n",
        "\n",
        "    else:\n",
        "        # Regular square: move in all directions equally\n",
        "        for action in actions:\n",
        "            next_state, reward = step(s, action)\n",
        "            p_pi[s, next_state] += 0.25\n",
        "            r_pi[s] += 0.25 * reward\n",
        "\n"
      ],
      "metadata": {
        "id": "URU9lAquaZGY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import solve\n",
        "\n",
        "identity = np.eye(num_states)\n",
        "\n",
        "# Solve the equation to get the value of each state\n",
        "a = identity - gamma * p_pi\n",
        "v_pi = solve(a, r_pi)\n",
        "\n"
      ],
      "metadata": {
        "id": "t_gd_AVGaZ1z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "coojsG-1acuT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterative_policy_evaluation(theta=1e-6, gamma=gamma, max_iterations=1000):\n",
        "    \"\"\"\n",
        "    Repeatedly updates the value function using a uniform random policy.\n",
        "    Stops when the values stop changing much (less than theta).\n",
        "    \"\"\"\n",
        "    v = np.zeros(num_states)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        delta = 0\n",
        "        new_v = np.zeros_like(v)\n",
        "\n",
        "        for s in range(num_states):\n",
        "            if s == blue:\n",
        "                # Blue always goes to red and gives +5\n",
        "                new_v[s] = 5 + gamma * v[red]\n",
        "\n",
        "            elif s == green:\n",
        "                # Green jumps to red or yellow randomly\n",
        "                new_v[s] = 2.5 + gamma * 0.5 * (v[red] + v[yellow])\n",
        "\n",
        "            else:\n",
        "                expected_value = 0\n",
        "                for action in actions:\n",
        "                    next_state, reward = step(s, action)\n",
        "                    expected_value += 0.25 * (reward + gamma * v[next_state])\n",
        "                new_v[s] = expected_value\n",
        "\n",
        "            delta = max(delta, abs(new_v[s] - v[s]))\n",
        "\n",
        "        v = new_v\n",
        "\n",
        "        if delta < theta:\n",
        "            print(f\"Converged after {i + 1} iterations.\")\n",
        "            break\n",
        "\n",
        "    return v\n",
        "\n"
      ],
      "metadata": {
        "id": "XM4VDJvHamXG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_value_matrix(matrix, title=\"Value Function\"):\n",
        "    print(f\"\\n{title}:\\n\")\n",
        "    for r in range(grid_size):\n",
        "        row = matrix[r * grid_size:(r + 1) * grid_size]\n",
        "        print(\" \".join(f\"{val:6.2f}\" for val in row))"
      ],
      "metadata": {
        "id": "gpoUZDB2M0JM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v_iter = iterative_policy_evaluation()\n",
        "\n",
        "print(\"Value function from iterative policy evaluation:\\n\")\n",
        "print_value_matrix(v_iter)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF_2e-hRam2q",
        "outputId": "997f698b-f5af-477f-ab01-5f9f3e8d533e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 149 iterations.\n",
            "Value function from iterative policy evaluation:\n",
            "\n",
            "\n",
            "Value Function:\n",
            "\n",
            "  2.07   4.51   2.39   2.36   0.73\n",
            "  1.11   1.79   1.34   1.00   0.30\n",
            "  0.23   0.56   0.46   0.21  -0.23\n",
            " -0.41  -0.12  -0.15  -0.35  -0.71\n",
            " -0.89  -0.51  -0.63  -0.84  -1.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1 part 2.1"
      ],
      "metadata": {
        "id": "_qgp1oLZaovP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(theta=1e-6, gamma=gamma, max_iterations=1000):\n",
        "    \"\"\"\n",
        "    Runs value iteration to find the best value function and policy.\n",
        "    Stops when updates get really small.\n",
        "    \"\"\"\n",
        "    v = np.zeros(num_states)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        delta = 0\n",
        "        new_v = np.zeros_like(v)\n",
        "\n",
        "        for s in range(num_states):\n",
        "            if s == blue:\n",
        "                new_v[s] = 5 + gamma * v[red]\n",
        "\n",
        "            elif s == green:\n",
        "                new_v[s] = 2.5 + gamma * 0.5 * (v[red] + v[yellow])\n",
        "\n",
        "            else:\n",
        "                q_values = []\n",
        "                for action in actions:\n",
        "                    next_state, reward = step(s, action)\n",
        "                    q = reward + gamma * v[next_state]\n",
        "                    q_values.append(q)\n",
        "                new_v[s] = max(q_values)\n",
        "\n",
        "            delta = max(delta, abs(new_v[s] - v[s]))\n",
        "\n",
        "        v = new_v\n",
        "\n",
        "        if delta < theta:\n",
        "            print(f\"Value iteration converged after {i + 1} iterations.\")\n",
        "            break\n",
        "\n",
        "    # Build a policy that picks the best action in each state\n",
        "    policy = np.full((num_states,), '', dtype=object)\n",
        "\n",
        "    for s in range(num_states):\n",
        "        if s in (blue, green):\n",
        "            policy[s] = 'special'\n",
        "        else:\n",
        "            best_action = None\n",
        "            best_value = float('-inf')\n",
        "\n",
        "            for action in actions:\n",
        "                next_state, reward = step(s, action)\n",
        "                q = reward + gamma * v[next_state]\n",
        "\n",
        "                if q > best_value:\n",
        "                    best_value = q\n",
        "                    best_action = action\n",
        "\n",
        "            policy[s] = best_action\n",
        "\n",
        "    return v, policy\n",
        "\n"
      ],
      "metadata": {
        "id": "6-crFX5jbYIz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v_opt, policy_opt = value_iteration()\n",
        "\n",
        "print(\"Optimal value function:\\n\")\n",
        "print_value_matrix(v_opt)\n",
        "\n",
        "print(\"\\nOptimal policy per state:\\n\")\n",
        "policy_grid = np.empty((grid_size, grid_size), dtype=object)\n",
        "\n",
        "for row in range(grid_size):\n",
        "    for col in range(grid_size):\n",
        "        state = to_1d(row, col)\n",
        "        a = policy_opt[state]\n",
        "        if a == 'special':\n",
        "            policy_grid[row, col] = '**'\n",
        "        else:\n",
        "            policy_grid[row, col] = a[:2].upper()\n",
        "\n",
        "print(policy_grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hbQMF0wM-pw",
        "outputId": "65e4c8f6-cb14-4d1b-f58a-d5f44adf5ec2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value iteration converged after 302 iterations.\n",
            "Optimal value function:\n",
            "\n",
            "\n",
            "Value Function:\n",
            "\n",
            " 21.00  22.10  21.00  19.60  18.62\n",
            " 19.95  21.00  19.95  18.95  18.00\n",
            " 18.95  19.95  18.95  18.00  17.10\n",
            " 18.00  18.95  18.00  17.10  16.25\n",
            " 17.10  18.00  17.10  16.25  15.43\n",
            "\n",
            "Optimal policy per state:\n",
            "\n",
            "[['RI' '**' 'LE' '**' 'LE']\n",
            " ['UP' 'UP' 'UP' 'LE' 'LE']\n",
            " ['UP' 'UP' 'UP' 'UP' 'UP']\n",
            " ['UP' 'UP' 'UP' 'UP' 'UP']\n",
            " ['UP' 'UP' 'UP' 'UP' 'UP']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0aUcl0wDbYCk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1 part 2.2"
      ],
      "metadata": {
        "id": "4BVkwn6Qby0w"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(gamma=gamma, theta=1e-6, max_iterations=1000):\n",
        "    \"\"\"\n",
        "    Finds the best value function and policy using policy iteration.\n",
        "    It starts with a random policy and keeps improving it.\n",
        "    \"\"\"\n",
        "    # Initialise policy randomly\n",
        "    policy = np.random.choice(actions, size=num_states)\n",
        "    policy[blue] = 'special'\n",
        "    policy[green] = 'special'\n",
        "\n",
        "    v = np.zeros(num_states)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        #  Evaluate the current policy\n",
        "        while True:\n",
        "            delta = 0\n",
        "            new_v = np.zeros_like(v)\n",
        "\n",
        "            for s in range(num_states):\n",
        "                if policy[s] == 'special':\n",
        "                    if s == blue:\n",
        "                        new_v[s] = 5 + gamma * v[red]\n",
        "                    elif s == green:\n",
        "                        new_v[s] = 2.5 + gamma * 0.5 * (v[red] + v[yellow])\n",
        "                else:\n",
        "                    a = policy[s]\n",
        "                    next_state, reward = step(s, a)\n",
        "                    new_v[s] = reward + gamma * v[next_state]\n",
        "\n",
        "                delta = max(delta, abs(new_v[s] - v[s]))\n",
        "\n",
        "            v = new_v\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "        # Improve the policy\n",
        "        policy_stable = True\n",
        "\n",
        "        for s in range(num_states):\n",
        "            if s in (blue, green):\n",
        "                continue\n",
        "\n",
        "            old_action = policy[s]\n",
        "            best_action = None\n",
        "            best_value = float('-inf')\n",
        "\n",
        "            for a in actions:\n",
        "                next_state, reward = step(s, a)\n",
        "                q = reward + gamma * v[next_state]\n",
        "\n",
        "                if q > best_value:\n",
        "                    best_value = q\n",
        "                    best_action = a\n",
        "\n",
        "            policy[s] = best_action\n",
        "\n",
        "            if old_action != best_action:\n",
        "                policy_stable = False\n",
        "\n",
        "        if policy_stable:\n",
        "            print(f\"Policy iteration converged after {i + 1} iterations.\")\n",
        "            break\n",
        "\n",
        "    return v, policy\n"
      ],
      "metadata": {
        "id": "NdWYB0Zob0OQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "v_pi, policy_pi = policy_iteration()\n",
        "\n",
        "print(\"Value function from policy iteration:\\n\")\n",
        "print_value_matrix(v_pi)\n",
        "\n",
        "print(\"\\nOptimal policy from policy iteration:\\n\")\n",
        "\n",
        "policy_grid = np.empty((grid_size, grid_size), dtype=object)\n",
        "\n",
        "for row in range(grid_size):\n",
        "    for col in range(grid_size):\n",
        "        state = to_1d(row, col)\n",
        "        action = policy_pi[state]\n",
        "        if action == 'special':\n",
        "            policy_grid[row, col] = \"**\"\n",
        "        else:\n",
        "            policy_grid[row, col] = action[:2].upper()\n",
        "\n",
        "print(policy_grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2oh0vA3NLQX",
        "outputId": "c05ba2c7-b748-4037-ca74-883a91d0488e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy iteration converged after 5 iterations.\n",
            "Value function from policy iteration:\n",
            "\n",
            "\n",
            "Value Function:\n",
            "\n",
            " 21.00  22.10  21.00  19.60  18.62\n",
            " 19.95  21.00  19.95  18.95  18.00\n",
            " 18.95  19.95  18.95  18.00  17.10\n",
            " 18.00  18.95  18.00  17.10  16.25\n",
            " 17.10  18.00  17.10  16.25  15.43\n",
            "\n",
            "Optimal policy from policy iteration:\n",
            "\n",
            "[['RI' 'SP' 'LE' 'SP' 'LE']\n",
            " ['UP' 'UP' 'UP' 'LE' 'LE']\n",
            " ['UP' 'UP' 'UP' 'UP' 'UP']\n",
            " ['UP' 'UP' 'UP' 'UP' 'UP']\n",
            " ['UP' 'UP' 'UP' 'UP' 'UP']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cAH_fv2ucEhs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GOkaOg7-dDIg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ryy8fR5HdDGB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pTFhvMVdDDn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 2"
      ],
      "metadata": {
        "id": "J9t8g0GmdDwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EXtBWehVMbyv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "# Grid setup\n",
        "grid_size = 5\n",
        "gamma = 0.95\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "\n",
        "\n",
        "action_to_delta = {\n",
        "    'up': (-1, 0),\n",
        "    'down': (1, 0),\n",
        "    'left': (0, -1),\n",
        "    'right': (0, 1)\n",
        "}\n",
        "\n",
        "# Functions to switch between 2D grid and 1D state\n",
        "def to_1d(row, col):\n",
        "    return row * grid_size + col\n",
        "\n",
        "def to_2d(state):\n",
        "    return divmod(state, grid_size)\n",
        "\n",
        "# Special squares that cause jumps\n",
        "blue = to_1d(0, 1)\n",
        "green = to_1d(0, 4)\n",
        "\n",
        "# Jump targets\n",
        "red = to_1d(3, 2)\n",
        "yellow = to_1d(4, 4)\n",
        "\n",
        "# The episode ends if agent lands here\n",
        "terminal_states = {\n",
        "    to_1d(2, 0),\n",
        "    to_1d(2, 4),\n",
        "    to_1d(4, 0)\n",
        "}\n"
      ],
      "metadata": {
        "id": "hPOADZVrdG8d"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step(state, action):\n",
        "    \"\"\"\n",
        "    Takes an action from the current state.\n",
        "    Returns the new state, the reward, and whether the episode is over.\n",
        "    \"\"\"\n",
        "    if state in terminal_states:\n",
        "        return state, 0, True\n",
        "\n",
        "    row, col = to_2d(state)\n",
        "    dr, dc = action_to_delta[action]\n",
        "    new_row, new_col = row + dr, col + dc\n",
        "\n",
        "    # If move goes off the grid, stay in place and get a penalty\n",
        "    if new_row < 0 or new_row >= grid_size or new_col < 0 or new_col >= grid_size:\n",
        "        return state, -0.5, False\n",
        "\n",
        "    next_state = to_1d(new_row, new_col)\n",
        "\n",
        "    if state == blue:\n",
        "        return red, 5.0, False\n",
        "\n",
        "    # Handle green jump\n",
        "    if state == green:\n",
        "        jump_target = red if random.random() < 0.5 else yellow\n",
        "        return jump_target, 2.5, False\n",
        "\n",
        "    # Check if the move landed in a terminal state\n",
        "    if next_state in terminal_states:\n",
        "        return next_state, -0.2, True\n",
        "\n",
        "    # Normal move with small penalty\n",
        "    return next_state, -0.2, False\n"
      ],
      "metadata": {
        "id": "mVZMlk7rdJuJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_control_es(num_episodes=1000, gamma=gamma, alpha=0.1, max_steps=100):\n",
        "    \"\"\"\n",
        "    Monte Carlo control with exploring starts.\n",
        "    Returns Q-values and the learned greedy policy.\n",
        "    \"\"\"\n",
        "    # Set up Q-values and an initial random policy\n",
        "    q = {s: {a: 0.0 for a in actions} for s in range(grid_size * grid_size)}\n",
        "    policy = {s: random.choice(actions) for s in range(grid_size * grid_size)}\n",
        "\n",
        "    for s in terminal_states:\n",
        "        policy[s] = None\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        # Start from a random non terminal state and action\n",
        "        while True:\n",
        "            state = random.randint(0, grid_size * grid_size - 1)\n",
        "            if state not in terminal_states:\n",
        "                break\n",
        "        action = random.choice(actions)\n",
        "\n",
        "        # Run an episode\n",
        "        episode = []\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < max_steps:\n",
        "            next_state, reward, done = step(state, action)\n",
        "            episode.append((state, action, reward))\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            action = policy[next_state]\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "        # Update Q-values using first visit MC\n",
        "        G = 0\n",
        "        visited = set()\n",
        "\n",
        "        for t in reversed(range(len(episode))):\n",
        "            s, a, r = episode[t]\n",
        "            G = gamma * G + r\n",
        "\n",
        "            if (s, a) not in visited:\n",
        "                visited.add((s, a))\n",
        "\n",
        "                # Update Q using incremental MC estimate\n",
        "                q[s][a] += alpha * (G - q[s][a])\n",
        "\n",
        "                # Update policy to pick the best action\n",
        "                best_action = max(q[s], key=q[s].get)\n",
        "                policy[s] = best_action\n",
        "\n",
        "    return q, policy\n"
      ],
      "metadata": {
        "id": "LjsUQ3FNdMLj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "zyqJ5uQxNpO5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run MC control with exploring starts\n",
        "q_es, policy_es = mc_control_es()\n",
        "\n",
        "# Set up matrices for value and policy\n",
        "v_matrix = np.zeros((grid_size, grid_size), dtype=np.float32)\n",
        "policy_matrix = np.empty((grid_size, grid_size), dtype=object)\n",
        "\n",
        "for r in range(grid_size):\n",
        "    for c in range(grid_size):\n",
        "        s = to_1d(r, c)\n",
        "\n",
        "        if s in terminal_states:\n",
        "            v_matrix[r, c] = 0.0\n",
        "            policy_matrix[r, c] = \"TT\"\n",
        "\n",
        "        elif s == blue:\n",
        "            # Value from jumping to red with +5\n",
        "            v_matrix[r, c] = 5.0 + gamma * max(q_es[red].values())\n",
        "            policy_matrix[r, c] = \"SP\" # Special jump\n",
        "\n",
        "        elif s == green:\n",
        "            # Use value implied by special green transition\n",
        "            red_val = max(q_es[red].values())\n",
        "            yellow_val = max(q_es[yellow].values())\n",
        "            v_matrix[r, c] = 2.5 + gamma * 0.5 * (red_val + yellow_val)\n",
        "            policy_matrix[r, c] = \"SP\"\n",
        "\n",
        "        else:\n",
        "            # Use the best Q-value and the corresponding action\n",
        "            v_matrix[r, c] = max(q_es[s].values())\n",
        "            policy_matrix[r, c] = policy_es[s][:2].upper()\n",
        "\n",
        "v_matrix = np.round(v_matrix, 2)\n",
        "\n",
        "print(\"\\n State-Value Function Matrix:\")\n",
        "print(v_matrix)\n",
        "\n",
        "print(\"\\n Policy Matrix:\")\n",
        "print(policy_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCbVvEXp5l0p",
        "outputId": "922cdde5-57ec-4edd-c11c-4bf4f88628ad"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " State-Value Function Matrix:\n",
            "[[ 1.19  4.71 -0.34 -0.47  2.12]\n",
            " [-0.11 -0.38 -0.28 -0.38 -0.18]\n",
            " [ 0.   -0.2  -0.4  -0.2   0.  ]\n",
            " [-0.17 -0.28 -0.31 -0.39 -0.2 ]\n",
            " [ 0.   -0.2  -0.37 -0.57 -0.49]]\n",
            "\n",
            " Policy Matrix:\n",
            "[['RI' 'SP' 'DO' 'UP' 'SP']\n",
            " ['DO' 'DO' 'UP' 'DO' 'DO']\n",
            " ['TT' 'LE' 'LE' 'RI' 'TT']\n",
            " ['DO' 'RI' 'LE' 'RI' 'UP']\n",
            " ['TT' 'LE' 'LE' 'UP' 'DO']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2.1.2 â€” Monte Carlo Control with epsilon-soft Policy\n"
      ],
      "metadata": {
        "id": "YsZEvY_HifO4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_control_eps_soft(num_episodes=5000, gamma=gamma, alpha=0.1, epsilon=0.1, max_steps=100):\n",
        "    \"\"\"\n",
        "    Monte Carlo control using an epsilon-soft policy.\n",
        "    Returns the learned Q-values and greedy policy.\n",
        "    \"\"\"\n",
        "    # Initialise Q-values for all state-action pairs\n",
        "    q = {s: {a: 0.0 for a in actions} for s in range(grid_size * grid_size)}\n",
        "\n",
        "    def get_action_probabilities(state):\n",
        "        \"\"\"\n",
        "        Returns action probabilities using epsilon soft strategy.\n",
        "        \"\"\"\n",
        "        values = q[state]\n",
        "        best_action = max(values, key=values.get)\n",
        "        probs = {a: epsilon / len(actions) for a in actions}\n",
        "        probs[best_action] += 1.0 - epsilon\n",
        "        return probs\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        # Pick a random starting state that is not terminal\n",
        "        while True:\n",
        "            state = random.randint(0, grid_size * grid_size - 1)\n",
        "            if state not in terminal_states:\n",
        "                break\n",
        "\n",
        "        episode = []\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < max_steps:\n",
        "          # Choose an action based on epsilon-soft probabilities\n",
        "            action_probs = get_action_probabilities(state)\n",
        "            action = random.choices(list(action_probs.keys()), weights=list(action_probs.values()))[0]\n",
        "\n",
        "            next_state, reward, done = step(state, action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "\n",
        "        G = 0\n",
        "        visited = set()\n",
        "\n",
        "        for t in reversed(range(len(episode))):\n",
        "            s, a, r = episode[t]\n",
        "            G = gamma * G + r\n",
        "\n",
        "            if (s, a) not in visited:\n",
        "                visited.add((s, a))\n",
        "                q[s][a] += alpha * (G - q[s][a])\n",
        "\n",
        "    # Final greedy policy from Q-values\n",
        "    final_policy = {}\n",
        "    for s in range(grid_size * grid_size):\n",
        "        if s in terminal_states:\n",
        "            final_policy[s] = None\n",
        "        elif s == blue or s == green:\n",
        "            final_policy[s] = 'special'\n",
        "        else:\n",
        "            final_policy[s] = max(q[s], key=q[s].get)\n",
        "\n",
        "    return q, final_policy\n"
      ],
      "metadata": {
        "id": "ba2B4h9WdW0-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run MC control with epsilon soft policy\n",
        "\n",
        "q_eps, policy_eps = mc_control_eps_soft()\n",
        "\n",
        "# Set up the value and policy grids\n",
        "v_matrix = np.zeros((grid_size, grid_size), dtype=np.float32)\n",
        "policy_matrix = np.empty((grid_size, grid_size), dtype=object)\n",
        "\n",
        "for r in range(grid_size):\n",
        "    for c in range(grid_size):\n",
        "        s = to_1d(r, c)\n",
        "\n",
        "        if s in terminal_states:\n",
        "            v_matrix[r, c] = 0.0\n",
        "            policy_matrix[r, c] = \"TT\"\n",
        "        elif s == blue:\n",
        "          # Value comes from jumping to red with reward\n",
        "            v_matrix[r, c] = 5.0 + gamma * v_matrix[to_2d(red)]\n",
        "            policy_matrix[r, c] = \"SP\"\n",
        "        elif s == green:\n",
        "          # Average value from jumping to red or yellow\n",
        "            v_matrix[r, c] = 2.5 + gamma * 0.5 * (\n",
        "                v_matrix[to_2d(red)] + v_matrix[to_2d(yellow)]\n",
        "            )\n",
        "            policy_matrix[r, c] = \"SP\"\n",
        "        else:\n",
        "          # Take the best Q-value and action\n",
        "            v_matrix[r, c] = max(q_eps[s].values())\n",
        "            policy_matrix[r, c] = policy_eps[s][:2].upper()\n",
        "\n",
        "v_matrix = np.round(v_matrix, 2)\n",
        "\n",
        "print(\"\\n State-Value Function Matrix (epsilon-soft):\")\n",
        "print(v_matrix)\n",
        "\n",
        "print(\"\\n Policy Matrix (epsilon-soft):\")\n",
        "for row in policy_matrix:\n",
        "    print(\" \".join(f\"{a:>4}\" for a in row))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uThdrlGL6xZb",
        "outputId": "ad62a817-7e65-4609-9d22-aa81a070f118"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " State-Value Function Matrix (epsilon-soft):\n",
            "[[ 3.82  5.    3.63  3.1   2.5 ]\n",
            " [ 3.14  3.78  3.11  2.51  1.64]\n",
            " [ 0.   -0.2   1.5   1.6   0.  ]\n",
            " [-0.2  -0.39 -0.58 -0.41 -0.2 ]\n",
            " [ 0.   -0.2  -0.43 -0.57 -0.39]]\n",
            "\n",
            " Policy Matrix (epsilon-soft):\n",
            "  RI   SP   LE   LE   SP\n",
            "  RI   UP   LE   UP   UP\n",
            "  TT   LE   UP   LE   TT\n",
            "  UP   LE   LE   RI   UP\n",
            "  TT   LE   LE   RI   UP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2.2  Off-Policy Monte Carlo Control with Importance Sampling"
      ],
      "metadata": {
        "id": "mBgSNlVhikYM"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_off_policy_control(num_episodes=5000, gamma=gamma, max_steps=100):\n",
        "    \"\"\"\n",
        "    Off-policy Monte Carlo control using importance sampling.\n",
        "    Learns a greedy policy while generating episodes from a uniform behaviour policy.\n",
        "    \"\"\"\n",
        "    # Set up Q-values and tracking weights\n",
        "    q = {s: {a: 0.0 for a in actions} for s in range(grid_size * grid_size)}\n",
        "    c = {s: {a: 0.0 for a in actions} for s in range(grid_size * grid_size)}\n",
        "\n",
        "    # Start with a random target policy\n",
        "    target_policy = {s: random.choice(actions) for s in range(grid_size * grid_size)}\n",
        "    for s in terminal_states:\n",
        "        target_policy[s] = None\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        # Pick a random non terminal starting state\n",
        "        while True:\n",
        "            state = random.randint(0, grid_size * grid_size - 1)\n",
        "            if state not in terminal_states:\n",
        "                break\n",
        "\n",
        "        # Generate an episode\n",
        "        episode = []\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < max_steps:\n",
        "            action = random.choice(actions)\n",
        "            next_state, reward, done = step(state, action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "        # Work backwards through the episode using importance sampling\n",
        "        G = 0\n",
        "        W = 1\n",
        "\n",
        "        for t in reversed(range(len(episode))):\n",
        "            s, a, r = episode[t]\n",
        "            G = gamma * G + r\n",
        "\n",
        "            c[s][a] += W\n",
        "            q[s][a] += (W / c[s][a]) * (G - q[s][a])\n",
        "\n",
        "            # Greedy policy update\n",
        "            best_action = max(q[s], key=q[s].get)\n",
        "            target_policy[s] = best_action\n",
        "\n",
        "            # Stop if action differs from the target policy\n",
        "            if a != best_action:\n",
        "                break\n",
        "\n",
        "            # Update weight based on behaviour policy (1/0.25 for uniform)\n",
        "            W *= 1 / 0.25\n",
        "\n",
        "    return q, target_policy\n"
      ],
      "metadata": {
        "id": "PJ6lw6ZEi8os"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RqZrC9AnPW6b"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Run off-policy MC control\n",
        "q_off, policy_off = mc_off_policy_control()\n",
        "\n",
        "# Create value and policy matrices\n",
        "v_matrix = np.zeros((grid_size, grid_size), dtype=np.float32)\n",
        "policy_matrix = np.empty((grid_size, grid_size), dtype=object)\n",
        "\n",
        "for r in range(grid_size):\n",
        "    for c in range(grid_size):\n",
        "        s = to_1d(r, c)\n",
        "\n",
        "        if s in terminal_states:\n",
        "            v_matrix[r, c] = 0.0\n",
        "            policy_matrix[r, c] = \"TT\"\n",
        "        elif s == blue:\n",
        "            v_matrix[r, c] = 5.0 + gamma * v_matrix[to_2d(red)]\n",
        "            policy_matrix[r, c] = \"SP\"\n",
        "        elif s == green:\n",
        "            v_matrix[r, c] = 2.5 + gamma * 0.5 * (\n",
        "                v_matrix[to_2d(red)] + v_matrix[to_2d(yellow)]\n",
        "            )\n",
        "            policy_matrix[r, c] = \"SP\"\n",
        "        else:\n",
        "            v_matrix[r, c] = max(q_off[s].values())\n",
        "            policy_matrix[r, c] = policy_off[s][:2].upper()\n",
        "\n",
        "\n",
        "v_matrix = np.round(v_matrix, 2)\n",
        "\n",
        "print(\"\\n State-Value Function Matrix (Off-Policy):\")\n",
        "print(v_matrix)\n",
        "\n",
        "print(\"\\n Policy Matrix (Off-Policy):\")\n",
        "for row in policy_matrix:\n",
        "    print(\" \".join(f\"{a:>4}\" for a in row))\n"
      ],
      "metadata": {
        "id": "qgAJlrLhjBeW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15dc9eb4-ef57-4f0c-84ce-997c83a78922"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " State-Value Function Matrix (Off-Policy):\n",
            "[[0.   5.   0.   0.   2.5 ]\n",
            " [0.   4.55 0.   0.   0.  ]\n",
            " [0.   4.12 3.72 0.   0.  ]\n",
            " [0.   0.   3.33 2.96 0.  ]\n",
            " [0.   0.   0.   2.62 2.29]]\n",
            "\n",
            " Policy Matrix (Off-Policy):\n",
            "  RI   SP   UP   RI   SP\n",
            "  UP   UP   DO   UP   UP\n",
            "  TT   UP   LE   UP   TT\n",
            "  LE   UP   UP   LE   DO\n",
            "  TT   UP   LE   UP   LE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HHP1QdBv7IkX"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}